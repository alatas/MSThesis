{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-C7OTd7nwnz",
    "outputId": "407c1656-7f57-450c-b608-a2d6b275a891"
   },
   "outputs": [],
   "source": [
    "use_google_drive = False\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import drive\n",
    "  !pip install webdataset\n",
    "  use_google_drive = True\n",
    "except Exception:\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqNaCZACzVjt",
    "outputId": "b96b275f-d243-4fb5-d114-e3cd0d5532d1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import webdataset as wds\n",
    "from itertools import islice\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset_4 = True\n",
    "write_dataset_5 = True\n",
    "\n",
    "write_fulldataset = True\n",
    "write_splitdatasets = True\n",
    "\n",
    "max_image_count = 0 #set 0 for all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_google_drive:\n",
    "  dataset_file = \"file:///content/gdrive/MyDrive/ColabData/amazon/shoes-224-full-3.tar\"\n",
    "  metadata_folder = \"/content/gdrive/MyDrive/ColabData/amazon/\"\n",
    "  dataset_folder = \"/content/gdrive/MyDrive/ColabData/amazon/\"\n",
    "\n",
    "  drive.mount(\"/content/gdrive\")\n",
    "else:\n",
    "  dataset_file = f\"file://{os.getcwd()}/dataset/shoes-224-full-3.tar\".replace('\\\\', '/')\n",
    "  metadata_folder = \"metadata\\\\\"\n",
    "  dataset_folder = \"dataset\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76JzZgy5adZT"
   },
   "outputs": [],
   "source": [
    "classes = [\"black\", \"white\", \"gray\", \"red\", \"green\", \"blue\", \"orange\", \"purple\", \"yellow\", \"pink\", \"brown\", \"multicolor\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_imadd(index, f, img, label, color):\n",
    "  f.add_subplot(10, 10, index)\n",
    "\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')\n",
    "  plt.title(label, loc='left', fontdict={\"color\": color})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_canvas(dataset, include=[], exclude=[], show_class=0):\n",
    "  f = plt.figure(figsize=(10, 10), dpi=200)\n",
    "  plt.rcParams.update({'font.size': 4})\n",
    "  plt.rcParams.update({'lines.linewidth': 0.5})\n",
    "  plt.subplots_adjust(wspace=-0.6, hspace=0.6)\n",
    "  index = 1\n",
    "  images = []\n",
    "\n",
    "  for jpg, cls, key in dataset:\n",
    "    if not show_class == int(cls):\n",
    "      continue\n",
    "\n",
    "    if key in exclude:\n",
    "      continue\n",
    "\n",
    "    if index <= 100 and (len(include) == 0 or (key in include)):\n",
    "      color = \"black\"\n",
    "      label = f\"\"\"{index}\"\"\"\n",
    "      torch_imadd(index, f, jpg, label, color)\n",
    "      index += 1\n",
    "      images.append(key)\n",
    "\n",
    "  return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtolist(inputlist: list, *items):\n",
    "  for item in items:\n",
    "    if not list[item - 1] in inputlist:\n",
    "      inputlist.append(list[item - 1])\n",
    "\n",
    "\n",
    "def savelisttofile(inputlist: list, filename):\n",
    "  with open(f'{metadata_folder}{filename}', 'w') as fp:\n",
    "    for item in inputlist:\n",
    "      fp.write(f\"{item}\\n\")\n",
    "\n",
    "\n",
    "def loadlistfromfile(outputlist: list, filename):\n",
    "  with open(f'{metadata_folder}{filename}', 'r') as fp:\n",
    "    for item in fp.readlines():\n",
    "      outputlist.append(item.rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "H3RruVj_eGlT",
    "outputId": "99f94694-4f61-482e-a95b-0a05b1b93d4e"
   },
   "outputs": [],
   "source": [
    "dataset = (wds.WebDataset(dataset_file)\n",
    "           .decode(\"pil\", only=\"jpg\")\n",
    "           .to_tuple(\"jpg\", \"cls\", \"__key__\")\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw On Canvas and Verify Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. reset on every beginning on every class\n",
    "cls = 11\n",
    "invalid = []\n",
    "misclassified = []\n",
    "list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. draw the new 100 images, if all of them is good go to 4, else goto 3\n",
    "exclude = invalid + misclassified\n",
    "list = draw_canvas(dataset, exclude=exclude, show_class=cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. remove the invalid or misclassified images by adding them to list, goto 2\n",
    "#addtolist(invalid, 100)\n",
    "#addtolist(misclassified,100)\n",
    "\n",
    "\n",
    "addtolist(misclassified, 98, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. persist list to disk and finish\n",
    "savelisttofile(list, f\"expert_{cls}_valid.txt\")\n",
    "savelisttofile(invalid, f\"expert_{cls}_invalid.txt\")\n",
    "savelisttofile(misclassified, f\"expert_{cls}_misclassified.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. draw final lists to canvas\n",
    "_ = draw_canvas(dataset, include=invalid, show_class=cls)\n",
    "_ = draw_canvas(dataset, include=misclassified, show_class=cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "valids = []\n",
    "invalids = []\n",
    "misclassifieds = []\n",
    "\n",
    "for cls in range(12):\n",
    "  loadlistfromfile(valids, f\"expert_{cls}_valid.txt\")\n",
    "  loadlistfromfile(invalids, f\"expert_{cls}_invalid.txt\")\n",
    "  loadlistfromfile(misclassifieds, f\"expert_{cls}_misclassified.txt\")\n",
    "\n",
    "  for item in valids:\n",
    "    combined[item] = 'valid'\n",
    "\n",
    "  for item in invalids:\n",
    "    combined[item] = 'invalid'\n",
    "\n",
    "  for item in misclassifieds:\n",
    "    combined[item] = 'misclassified'\n",
    "\n",
    "\n",
    "savelisttofile(valids, f\"expert_all_valid.txt\")\n",
    "savelisttofile(invalids, f\"expert_all_invalid.txt\")\n",
    "savelisttofile(misclassifieds, f\"expert_all_misclassified.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previously Saved Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "valids = []\n",
    "invalids = []\n",
    "misclassifieds = []\n",
    "\n",
    "loadlistfromfile(valids, f\"expert_all_valid.txt\")\n",
    "loadlistfromfile(invalids, f\"expert_all_invalid.txt\")\n",
    "loadlistfromfile(misclassifieds, f\"expert_all_misclassified.txt\")\n",
    "\n",
    "\n",
    "for item in valids:\n",
    "  combined[item] = 'valid'\n",
    "\n",
    "for item in invalids:\n",
    "  combined[item] = 'invalid'\n",
    "\n",
    "for item in misclassifieds:\n",
    "  combined[item] = 'misclassified'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_metadata(sample: dict):\n",
    "  key = sample[\"__key__\"]\n",
    "  if key in combined.keys():\n",
    "    sample[\"__decision__\"] = combined[key]\n",
    "  else:\n",
    "    sample[\"__decision__\"] = None\n",
    "\n",
    "  return sample\n",
    "\n",
    "def convert_decision_to_cls(sample: dict):\n",
    "  if sample[\"__decision__\"] == 'invalid':\n",
    "    sample[\"cls\"] = 0\n",
    "  elif sample[\"__decision__\"] == 'valid':\n",
    "    sample[\"cls\"] = 1\n",
    "  else:\n",
    "    raise Exception(\"decision not defined\")\n",
    "\n",
    "  return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = (wds.WebDataset(dataset_file)\n",
    "             .map(match_metadata)\n",
    "             .select(predicate=lambda r: r[\"__decision__\"] == 'valid')\n",
    "             .decode(\"pil\", only=\"jpg\")\n",
    "             .to_tuple(\"__key__\", \"jpg\", \"cls\", \"sort\")\n",
    "             .shuffle(100)\n",
    "             )\n",
    "\n",
    "dataset_5 = (wds.WebDataset(dataset_file)\n",
    "             .map(match_metadata)\n",
    "             .select(predicate=lambda r: r[\"__decision__\"] == 'invalid' or r[\"__decision__\"] == 'valid')\n",
    "             .map(convert_decision_to_cls)\n",
    "             .decode(\"pil\", only=\"jpg\")\n",
    "             .to_tuple(\"__key__\", \"jpg\", \"cls\", \"sort\")\n",
    "             .shuffle(100)\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, jpg, cls, sort in dataset_4:\n",
    "  break\n",
    "\n",
    "print(classes[int(cls)])\n",
    "print(f\"sort:{int(sort, 2)}\")\n",
    "plt.imshow(jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, jpg, cls, sort in dataset_5:\n",
    "  break\n",
    "\n",
    "print(\"valid\" if int(cls) == 1 else \"invalid\")\n",
    "plt.imshow(jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcChLuMdTc-u"
   },
   "outputs": [],
   "source": [
    "def write2TARs(dataset_id, dataset, folder):\n",
    "\n",
    "  filefull = os.path.join(folder, f\"shoes-224-full-{dataset_id}.tar\")\n",
    "  filetraining = os.path.join(folder, f\"shoes-224-training-{dataset_id}.tar\")\n",
    "  filevalidation = os.path.join(folder, f\"shoes-224-validation-{dataset_id}.tar\")\n",
    "  filetest = os.path.join(folder, f\"shoes-224-test-{dataset_id}.tar\")\n",
    "\n",
    "  full_size, training_size, validation_size, test_size = 0, 0, 0, 0\n",
    "  with wds.TarWriter(filefull) as full, wds.TarWriter(filetraining) as train, wds.TarWriter(filevalidation) as validation, wds.TarWriter(filetest) as test:\n",
    "    i = 0\n",
    "    for item in dataset:\n",
    "\n",
    "      item = {\n",
    "          \"__key__\": item[0],\n",
    "          \"jpg\": item[1],\n",
    "          \"cls\": item[2],\n",
    "          \"sort\": item[3]\n",
    "      }\n",
    "\n",
    "      if write_fulldataset and dataset_id != 5: #dataset 5 is only used for CNN\n",
    "        full.write(item)\n",
    "        full_size += 1\n",
    "\n",
    "      if write_splitdatasets:\n",
    "        if i % 10 < 6:\n",
    "          train.write(item)\n",
    "          training_size += 1\n",
    "        elif i % 10 < 8:\n",
    "          validation.write(item)\n",
    "          validation_size += 1\n",
    "        else:\n",
    "          test.write(item)\n",
    "          test_size += 1\n",
    "\n",
    "      i += 1\n",
    "      if max_image_count > 0 and i >= max_image_count:\n",
    "        break\n",
    "\n",
    "  return full_size, training_size, validation_size, test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJN5cQKOnktK"
   },
   "outputs": [],
   "source": [
    "if write_dataset_4:\n",
    "  results = write2TARs(4, dataset_4, dataset_folder)\n",
    "\n",
    "  print(f\"\"\"Dataset 4 is written to files:\n",
    "\n",
    "  # of Total images: {results[0]}\n",
    "\n",
    "  # of Training images: {results[1]}\n",
    "  # of Validation images: {results[2]}\n",
    "  # of Test images: {results[3]}\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "if write_dataset_5:\n",
    "  results = write2TARs(5, dataset_5, dataset_folder)\n",
    "\n",
    "  print(f\"\"\"Dataset 5 is written to files:\n",
    "\n",
    "  # of Training images: {results[1]}\n",
    "  # of Validation images: {results[2]}\n",
    "  # of Test images: {results[3]}\n",
    "  \"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
